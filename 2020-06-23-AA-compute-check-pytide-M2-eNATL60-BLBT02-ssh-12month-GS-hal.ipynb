{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "import dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='background-color: #f2f2f2; display: inline-block; padding: 10px; border: 1px solid #999999;'>\n",
       "  <h3>PBSCluster</h3>\n",
       "  <ul>\n",
       "    <li><b>Dashboard: </b><a href='http://10.120.41.90:8787/status' target='_blank'>http://10.120.41.90:8787/status</a>\n",
       "  </ul>\n",
       "</div>\n"
      ],
      "text/plain": [
       "PBSCluster('tcp://10.120.41.90:40718', workers=0, threads=0, memory=0 B)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster = PBSCluster(\n",
    "    cores=1,\n",
    "    memory=\"120GB\",\n",
    "    project='pangeo',\n",
    "    processes=1,\n",
    "    walltime='04:00:00',\n",
    "    local_directory='$TMPDIR')\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.120.41.90:40718</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.120.41.90:8787/status' target='_blank'>http://10.120.41.90:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.120.41.90:40718' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster) # scheduler_file=\"/home/ad/briolf/scheduler.json\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(client.scheduler_info()[\"workers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "!qstat -u albert7a\n",
    "\n",
    "import time\n",
    "nb_workers = 0\n",
    "while True:\n",
    "    nb_workers = len(client.scheduler_info()[\"workers\"])\n",
    "    if nb_workers >= 30:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "print(nb_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import netCDF4\n",
    "import pytide\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.35 s, sys: 114 ms, total: 2.47 s\n",
      "Wall time: 5.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "root = \"/work/ALT/odatis/eNATL60/BLBT02/gridT-2D/\"\n",
    "files = sorted(glob.glob(root+'/*.nc'))\n",
    "\n",
    "drop_vars = [\n",
    "    'nav_lat',\n",
    "    'nav_lon',\n",
    "    'somxl010',\n",
    "    'sosaline',\n",
    "    'sosstsst']\n",
    "\n",
    "# these are variables I want to drop while running `open_mfdataset` but then add back later\n",
    "extra_coord_vars = ['y', 'x']\n",
    "extra_coord_vars = []\n",
    "\n",
    "chunks = dict(time_counter=1)\n",
    "\n",
    "open_kwargs = dict(drop_variables=(drop_vars + extra_coord_vars),\n",
    "                   chunks=chunks, decode_cf=True, concat_dim=\"time_counter\") #, combine='nested')\n",
    "ds = xr.open_mfdataset(files, combine='nested',parallel=True, **open_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsg=xr.open_dataset('/work/ALT/odatis/eNATL60/mesh_mask_eNATL60_3.6_lev1.nc4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "box=(-90,-60,30,45)\n",
    "domaineNATL60=(box[0]<dsg.nav_lon)*(dsg.nav_lon<box[1])*(box[2]<dsg.nav_lat)*(dsg.nav_lat<box[3])\n",
    "whereeNATL60=np.where(domaineNATL60)\n",
    "jmineNATL60 = whereeNATL60[0].min()\n",
    "jmaxeNATL60 = whereeNATL60[0].max()\n",
    "imineNATL60 = whereeNATL60[1].min()\n",
    "imaxeNATL60 = whereeNATL60[1].max()\n",
    "\n",
    "START_DATE = np.datetime64('2009-07-01')\n",
    "END_DATE = np.datetime64('2010-06-30')\n",
    "time_series=ds['time_counter']\n",
    "period = (time_series >= START_DATE) & (time_series <= END_DATE)\n",
    "time=time_series[period]\n",
    "ssh=ds.sossheig[period,jmineNATL60:jmaxeNATL60+1,imineNATL60:imaxeNATL60+1]\n",
    "t=time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.9 ms, sys: 7 Âµs, total: 6.91 ms\n",
      "Wall time: 6.19 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wt = pytide.WaveTable()\n",
    "wt = pytide.WaveTable([\"M2\"])\n",
    "f, vu = wt.compute_nodal_modulations(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dask_array_rechunk(da, axis=0):\n",
    "    \"\"\"Search for the optimal block cutting without modifying the axis 'axis'\n",
    "    in order to optimize its access in memory.\"\"\"\n",
    "    nblocks = 1\n",
    "    \n",
    "    def calculate_chuncks_size(chunks, size):\n",
    "        result = np.array(chunks).prod() * size\n",
    "        return result / (1000**2)\n",
    "       \n",
    "    while True:\n",
    "        chunks = []\n",
    "        div = int(np.sqrt(nblocks))\n",
    "        for index, item in enumerate(da.chunks):\n",
    "            chunks.append(np.array(item).sum() * (div if index == axis else 1))\n",
    "        chunks = tuple(item // div for index, item in enumerate(chunks))\n",
    "        chuncks_size = calculate_chuncks_size(chunks, da.dtype.itemsize)\n",
    "        if chuncks_size > 100 and chuncks_size < 150:\n",
    "            return chunks\n",
    "        nblocks += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_along_axis(arr, func1d, func1d_axis, func1d_args, func1d_kwargs):\n",
    "    \"\"\"Wrap apply_along_axis\"\"\"\n",
    "    return np.apply_along_axis(func1d, func1d_axis, arr, *func1d_args,\n",
    "                                  **func1d_kwargs)\n",
    "\n",
    "\n",
    "def apply_along_axis(func1d, axis, arr, *args, **kwargs):\n",
    "    \"\"\"Apply the harmonic analysis to 1-D slices along the given axis.\"\"\"\n",
    "    arr = dask.array.core.asarray(arr)\n",
    "\n",
    "    # Validate and normalize axis.\n",
    "    arr.shape[axis]\n",
    "    axis = len(arr.shape[:axis])\n",
    "\n",
    "    # Rechunk so that analyze is applied over the full axis.\n",
    "    arr = arr.rechunk(arr.chunks[:axis] + (arr.shape[axis:axis + 1], ) +\n",
    "                      arr.chunks[axis + 1:])\n",
    "\n",
    "    # Test out some data with the function.\n",
    "    test_data = np.ones(args[0].shape[1], dtype=arr.dtype)\n",
    "    test_result = np.array(func1d(test_data, *args, **kwargs))\n",
    "\n",
    "    # Map analyze over the data to get the result\n",
    "    # Adds other axes as needed.\n",
    "    result = arr.map_blocks(\n",
    "        _apply_along_axis,\n",
    "        name=dask.utils.funcname(func1d) + '-along-axis',\n",
    "        dtype=test_result.dtype,\n",
    "        chunks=(arr.chunks[:axis] + test_result.shape + arr.chunks[axis + 1:]),\n",
    "        drop_axis=axis,\n",
    "        new_axis=list(range(axis, axis + test_result.ndim, 1)),\n",
    "        func1d=func1d,\n",
    "        func1d_axis=axis,\n",
    "        func1d_args=args,\n",
    "        func1d_kwargs=kwargs,\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_rechunk = ssh.chunk(dask_array_rechunk(ssh))\n",
    "future = apply_along_axis(pytide.WaveTable.harmonic_analysis, 0, ssh_rechunk,\n",
    "                          *(f, vu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70365"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(future.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 2.3 s, total: 1min 24s\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "analysis = future.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 163 ms, sys: 50.3 ms, total: 213 ms\n",
      "Wall time: 407 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "amp_da=xr.DataArray(np.absolute(analysis),dims={'freq','y','x'},name='tidal-amp')\n",
    "amp_da.to_netcdf(path='/work/ALT/odatis/eNATL60/outputs/pytide/eNATL60GS-BLBT02_tidal_amp_M2_y2009m07-y2010m06.nc',engine='scipy')\n",
    "\n",
    "phase_da=xr.DataArray(np.angle(analysis),dims={'freq','y','x'},name='tidal-phase')\n",
    "phase_da.to_netcdf(path='/work/ALT/odatis/eNATL60/outputs/pytide/eNATL60GS-BLBT02_tidal_phase_M2_y2009m07-y2010m06.nc',engine='scipy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 47s, sys: 2min 1s, total: 22min 49s\n",
      "Wall time: 22min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nt=len(time)\n",
    "nwaves, ni, nj = analysis.shape\n",
    "for k in np.arange(len(time)):\n",
    "    if not os.path.exists('/work/ALT/odatis/eNATL60/outputs/pytide/eNATL60GS-BLBT02_tide_M2_y2009m07-y2010m06_k'+str(k)+'.nc'):\n",
    "        ttide = wt.tide_from_mapping(time[k].astype('datetime64[s]').astype('float64'),analysis.reshape(nwaves, ni*nj)).reshape(ni, nj)\n",
    "        ttide_da=xr.DataArray(ttide,dims={'time_counter','y','x'},name='tide')\n",
    "        ttide_da.to_netcdf(path='/work/ALT/odatis/eNATL60/outputs/pytide/eNATL60GS-BLBT02_tide_M2_y2009m07-y2010m06_k'+str(k)+'.nc',unlimited_dims={'time_counter'},engine='scipy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytide",
   "language": "python",
   "name": "pytide"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
