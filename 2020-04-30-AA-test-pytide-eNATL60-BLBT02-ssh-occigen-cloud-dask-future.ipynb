{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:   tcp://10.32.16.73:43647\n",
      "distributed.scheduler - INFO -   dashboard at:                     :8787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28e3498632d40c894d0d33e355e48c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "from dask_kubernetes import KubeCluster\n",
    "cluster = KubeCluster(n_workers=20)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster.workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import netCDF4\n",
    "import pytide\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "fs = gcsfs.GCSFileSystem('pangeo-181919', requester_pays=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register tcp://10.32.11.7:46695\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.11.7:46695\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.11.6:42037\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.11.6:42037\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.7.7:43405\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.7.7:43405\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.7.8:46855\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.7.8:46855\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.17.6:39979\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.17.6:39979\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.10.7:44041\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.10.7:44041\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.17.7:34657\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.17.7:34657\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.10.6:46877\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.10.6:46877\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.6.6:39017\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.6.6:39017\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.6.7:34507\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.6.7:34507\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.9.7:45951\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.9.7:45951\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.9.6:36819\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.9.6:36819\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.12.6:33811\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.12.6:33811\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.12.7:36947\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.12.7:36947\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.5.7:36783\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.5.7:36783\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.8.7:44837\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.8.7:44837\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.5.6:37129\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.5.6:37129\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.8.8:39657\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.8.8:39657\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.45.6:43659\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.45.6:43659\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.32.45.7:37871\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.32.45.7:37871\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.7 s, sys: 196 ms, total: 1.9 s\n",
      "Wall time: 6.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "zmap = fs.get_mapper('pangeo-meom/eNATL60-BLBT02-SSH-1h')\n",
    "ds = xr.open_zarr(zmap)\n",
    "ds_sorted=ds.sortby('time_counter')\n",
    "ssh=ds_sorted['sossheig']\n",
    "time=ds_sorted['time_counter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = pytide.WaveTable()\n",
    "wt = pytide.WaveTable([\"M2\", \"S2\", \"N2\", \"O1\", \"K1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 ms, sys: 101 µs, total: 10.8 ms\n",
      "Wall time: 9.76 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f, vu = wt.compute_nodal_modulations(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dask_array_rechunk(da, axis=0):\n",
    "    \"\"\"Search for the optimal block cutting without modifying the axis 'axis'\n",
    "    in order to optimize its access in memory.\"\"\"\n",
    "    nblocks = 1\n",
    "    \n",
    "    def calculate_chuncks_size(chunks, size):\n",
    "        result = np.array(chunks).prod() * size\n",
    "        return result / (1000**2)\n",
    "       \n",
    "    while True:\n",
    "        chunks = []\n",
    "        div = int(np.sqrt(nblocks))\n",
    "        for index, item in enumerate(da.chunks):\n",
    "            chunks.append(np.array(item).sum() * (div if index == axis else 1))\n",
    "        chunks = tuple(item // div for index, item in enumerate(chunks))\n",
    "        chuncks_size = calculate_chuncks_size(chunks, da.dtype.itemsize)\n",
    "        if chuncks_size > 100 and chuncks_size < 150:\n",
    "            return chunks\n",
    "        nblocks += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_rechunk = ssh.chunk(dask_array_rechunk(ssh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_along_axis(arr, func1d, func1d_axis, func1d_args, func1d_kwargs):\n",
    "    \"\"\"Wrap apply_along_axis\"\"\"\n",
    "    return np.apply_along_axis(func1d, func1d_axis, arr, *func1d_args,\n",
    "                                  **func1d_kwargs)\n",
    "\n",
    "\n",
    "def apply_along_axis(func1d, axis, arr, *args, **kwargs):\n",
    "    \"\"\"Apply the harmonic analysis to 1-D slices along the given axis.\"\"\"\n",
    "    arr = dask.array.core.asarray(arr)\n",
    "\n",
    "    # Validate and normalize axis.\n",
    "    arr.shape[axis]\n",
    "    axis = len(arr.shape[:axis])\n",
    "\n",
    "    # Rechunk so that analyze is applied over the full axis.\n",
    "    arr = arr.rechunk(arr.chunks[:axis] + (arr.shape[axis:axis + 1], ) +\n",
    "                      arr.chunks[axis + 1:])\n",
    "\n",
    "    # Test out some data with the function.\n",
    "    test_data = np.ones(args[0].shape[1], dtype=arr.dtype)\n",
    "    test_result = np.array(func1d(test_data, *args, **kwargs))\n",
    "\n",
    "    # Map analyze over the data to get the result\n",
    "    # Adds other axes as needed.\n",
    "    result = arr.map_blocks(\n",
    "        _apply_along_axis,\n",
    "        name=dask.utils.funcname(func1d) + '-along-axis',\n",
    "        dtype=test_result.dtype,\n",
    "        chunks=(arr.chunks[:axis] + test_result.shape + arr.chunks[axis + 1:]),\n",
    "        drop_axis=axis,\n",
    "        new_axis=list(range(axis, axis + test_result.ndim, 1)),\n",
    "        func1d=func1d,\n",
    "        func1d_axis=axis,\n",
    "        func1d_args=args,\n",
    "        func1d_kwargs=kwargs,\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = apply_along_axis(pytide.WaveTable.harmonic_analysis, 0, ssh_rechunk,\n",
    "                          *(f, vu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = future.compute()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytide2",
   "language": "python",
   "name": "pytide2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
