{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad/alberta/git/conda-pack/pytide/lib/python3.6/site-packages/distributed/node.py:244: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 44912 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    }
   ],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "\n",
    "cluster = PBSCluster(\n",
    "    cores=1,\n",
    "    memory=\"120GB\",\n",
    "    project='pangeo',\n",
    "    processes=1,\n",
    "    walltime='00:30:00',\n",
    "    local_directory='$TMPDIR')\n",
    "cluster.scale(40)\n",
    "client = Client(cluster) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(client.scheduler_info()[\"workers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access /etc/dask: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /etc/dask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a8ce2de6355d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnb_workers\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!qstat -u albert7a\n",
    "\n",
    "import time\n",
    "nb_workers = 0\n",
    "while True:\n",
    "    nb_workers = len(client.scheduler_info()[\"workers\"])\n",
    "    if nb_workers >= 30:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "print(nb_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import netCDF4\n",
    "import pytide\n",
    "import xarray as xr\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "root = \"/work/ALT/odatis/eNATL60/BLBT02/gridT-2D/\"\n",
    "files = [\n",
    "    os.path.join(root, item) for item in os.listdir(root) if item.endswith(\".nc\")]\n",
    "\n",
    "drop_vars = [\n",
    "    'nav_lat',\n",
    "    'nav_lon',\n",
    "    'somxl010',\n",
    "    'sosaline',\n",
    "    'sosstsst']\n",
    "\n",
    "# these are variables I want to drop while running `open_mfdataset` but then add back later\n",
    "extra_coord_vars = ['time_counter', 'y', 'x']\n",
    "extra_coord_vars = []\n",
    "\n",
    "chunks = dict(time_counter=1)\n",
    "\n",
    "open_kwargs = dict(drop_variables=(drop_vars + extra_coord_vars),\n",
    "                   chunks=chunks, decode_cf=True, concat_dim=\"time_counter\") #, combine='nested')\n",
    "ds = xr.open_mfdataset(files, combine='nested',parallel=True, **open_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = np.datetime64('2009-07-01')\n",
    "END_DATE = np.datetime64('2009-07-31')\n",
    "time_series=ds['time_counter']\n",
    "period = (time_series >= START_DATE) & (time_series <= END_DATE)\n",
    "time=time_series[period]\n",
    "ssh=ds.sossheig[period]\n",
    "t=time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wt = pytide.WaveTable()\n",
    "wt = pytide.WaveTable([\"M2\", \"S2\", \"N2\", \"O1\", \"K1\"])f, vu = wt.compute_nodal_modulations(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dask_array_rechunk(da, axis=0):\n",
    "    \"\"\"Search for the optimal block cutting without modifying the axis 'axis'\n",
    "    in order to optimize its access in memory.\"\"\"\n",
    "    nblocks = 1\n",
    "    \n",
    "    def calculate_chuncks_size(chunks, size):\n",
    "        result = np.array(chunks).prod() * size\n",
    "        return result / (1000**2)\n",
    "       \n",
    "    while True:\n",
    "        chunks = []\n",
    "        div = int(np.sqrt(nblocks))\n",
    "        for index, item in enumerate(da.chunks):\n",
    "            chunks.append(np.array(item).sum() * (div if index == axis else 1))\n",
    "        chunks = tuple(item // div for index, item in enumerate(chunks))\n",
    "        chuncks_size = calculate_chuncks_size(chunks, da.dtype.itemsize)\n",
    "        if chuncks_size > 100 and chuncks_size < 150:\n",
    "            return chunks\n",
    "        nblocks += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_along_axis(arr, func1d, func1d_axis, func1d_args, func1d_kwargs):\n",
    "    \"\"\"Wrap apply_along_axis\"\"\"\n",
    "    return np.apply_along_axis(func1d, func1d_axis, arr, *func1d_args,\n",
    "                                  **func1d_kwargs)\n",
    "\n",
    "\n",
    "def apply_along_axis(func1d, axis, arr, *args, **kwargs):\n",
    "    \"\"\"Apply the harmonic analysis to 1-D slices along the given axis.\"\"\"\n",
    "    arr = dask.array.core.asarray(arr)\n",
    "\n",
    "    # Validate and normalize axis.\n",
    "    arr.shape[axis]\n",
    "    axis = len(arr.shape[:axis])\n",
    "\n",
    "    # Rechunk so that analyze is applied over the full axis.\n",
    "    arr = arr.rechunk(arr.chunks[:axis] + (arr.shape[axis:axis + 1], ) +\n",
    "                      arr.chunks[axis + 1:])\n",
    "\n",
    "    # Test out some data with the function.\n",
    "    test_data = np.ones(args[0].shape[1], dtype=arr.dtype)\n",
    "    test_result = np.array(func1d(test_data, *args, **kwargs))\n",
    "\n",
    "    # Map analyze over the data to get the result\n",
    "    # Adds other axes as needed.\n",
    "    result = arr.map_blocks(\n",
    "        _apply_along_axis,\n",
    "        name=dask.utils.funcname(func1d) + '-along-axis',\n",
    "        dtype=test_result.dtype,\n",
    "        chunks=(arr.chunks[:axis] + test_result.shape + arr.chunks[axis + 1:]),\n",
    "        drop_axis=axis,\n",
    "        new_axis=list(range(axis, axis + test_result.ndim, 1)),\n",
    "        func1d=func1d,\n",
    "        func1d_axis=axis,\n",
    "        func1d_args=args,\n",
    "        func1d_kwargs=kwargs,\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ssh_rechunk = ssh.chunk(dask_array_rechunk(ssh))\n",
    "future = apply_along_axis(pytide.WaveTable.harmonic_analysis, 0, ssh_rechunk,\n",
    "                          *(f, vu))\n",
    "analysis = future.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "analysis_da=xr.DataArray(analysis,dims={'freq','y','x'},name='tidal-analysis')\n",
    "analysis_da.to_netcdf.(path='/work/ALT/odatis/eNATL60/outputs/pytide/eNATL60-BLBT02_tidal_analysis_y2009m07.nc',mode='a',engine='scipy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nwaves, ni, nj = analysis.shape\n",
    "tide = wt.tide_from_mapping(\n",
    "    time[0].astype('datetime64[s]').astype('float64'),\n",
    "    analysis.reshape(nwaves, ni*nj)).reshape(ni, nj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tide_da=xr.DataArray(tide,dims={'time_counter','y','x'})\n",
    "tide_da.to_netcdf.(path='/work/ALT/odatis/eNATL60/outputs/pytide/eNATL60-BLBT02_tidal_reconstruct_y2009m07.nc',mode='a',engine='scipy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytide",
   "language": "python",
   "name": "pytide"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
