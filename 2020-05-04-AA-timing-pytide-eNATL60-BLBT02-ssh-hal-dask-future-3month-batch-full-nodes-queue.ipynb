{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd\n",
    "import dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad/alberta/git/conda-pack/pytide/lib/python3.6/site-packages/distributed/node.py:244: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 44902 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='background-color: #f2f2f2; display: inline-block; padding: 10px; border: 1px solid #999999;'>\n",
       "  <h3>PBSCluster</h3>\n",
       "  <ul>\n",
       "    <li><b>Dashboard: </b><a href='http://10.120.41.76:44902/status' target='_blank'>http://10.120.41.76:44902/status</a>\n",
       "  </ul>\n",
       "</div>\n"
      ],
      "text/plain": [
       "PBSCluster('tcp://10.120.41.76:40220', workers=0, threads=0, memory=0 B)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster = PBSCluster(\n",
    "    cores=1,\n",
    "    memory=\"120GB\",\n",
    "    project='pangeo',\n",
    "    processes=1,\n",
    "    walltime='04:00:00',\n",
    "    local_directory='$TMPDIR')\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.120.41.76:40220</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.120.41.76:44902/status' target='_blank'>http://10.120.41.76:44902/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.120.41.76:40220' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster) # scheduler_file=\"/home/ad/briolf/scheduler.json\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(client.scheduler_info()[\"workers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "!qstat -u albert7a\n",
    "\n",
    "import time\n",
    "nb_workers = 0\n",
    "while True:\n",
    "    nb_workers = len(client.scheduler_info()[\"workers\"])\n",
    "    if nb_workers >= 20:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "print(nb_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import netCDF4\n",
    "import pytide\n",
    "import xarray as xr\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.51 s, sys: 79.4 ms, total: 2.59 s\n",
      "Wall time: 5.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "root = \"/work/ALT/odatis/eNATL60/BLBT02/gridT-2D/\"\n",
    "files = [\n",
    "    os.path.join(root, item) for item in os.listdir(root) if item.endswith(\".nc\")]\n",
    "\n",
    "drop_vars = [\n",
    "    'nav_lat',\n",
    "    'nav_lon',\n",
    "    'somxl010',\n",
    "    'sosaline',\n",
    "    'sosstsst']\n",
    "\n",
    "# these are variables I want to drop while running `open_mfdataset` but then add back later\n",
    "extra_coord_vars = ['time_counter', 'y', 'x']\n",
    "extra_coord_vars = []\n",
    "\n",
    "chunks = dict(time_counter=1)\n",
    "\n",
    "open_kwargs = dict(drop_variables=(drop_vars + extra_coord_vars),\n",
    "                   chunks=chunks, decode_cf=True, concat_dim=\"time_counter\") #, combine='nested')\n",
    "ds = xr.open_mfdataset(files, combine='nested',parallel=True, **open_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = np.datetime64('2009-07-01')\n",
    "END_DATE = np.datetime64('2009-09-30')\n",
    "time_series=ds['time_counter']\n",
    "period = (time_series >= START_DATE) & (time_series <= END_DATE)\n",
    "time=time_series[period]\n",
    "ssh=ds.sossheig[period]\n",
    "t=time.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.55 ms, sys: 48 Âµs, total: 2.6 ms\n",
      "Wall time: 2.18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wt = pytide.WaveTable()\n",
    "wt = pytide.WaveTable([\"M2\", \"S2\", \"N2\", \"O1\", \"K1\"])\n",
    "f, vu = wt.compute_nodal_modulations(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dask_array_rechunk(da, axis=0):\n",
    "    \"\"\"Search for the optimal block cutting without modifying the axis 'axis'\n",
    "    in order to optimize its access in memory.\"\"\"\n",
    "    nblocks = 1\n",
    "    \n",
    "    def calculate_chuncks_size(chunks, size):\n",
    "        result = np.array(chunks).prod() * size\n",
    "        return result / (1000**2)\n",
    "       \n",
    "    while True:\n",
    "        chunks = []\n",
    "        div = int(np.sqrt(nblocks))\n",
    "        for index, item in enumerate(da.chunks):\n",
    "            chunks.append(np.array(item).sum() * (div if index == axis else 1))\n",
    "        chunks = tuple(item // div for index, item in enumerate(chunks))\n",
    "        chuncks_size = calculate_chuncks_size(chunks, da.dtype.itemsize)\n",
    "        if chuncks_size > 100 and chuncks_size < 150:\n",
    "            return chunks\n",
    "        nblocks += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_along_axis(arr, func1d, func1d_axis, func1d_args, func1d_kwargs):\n",
    "    \"\"\"Wrap apply_along_axis\"\"\"\n",
    "    return np.apply_along_axis(func1d, func1d_axis, arr, *func1d_args,\n",
    "                                  **func1d_kwargs)\n",
    "\n",
    "\n",
    "def apply_along_axis(func1d, axis, arr, *args, **kwargs):\n",
    "    \"\"\"Apply the harmonic analysis to 1-D slices along the given axis.\"\"\"\n",
    "    arr = dask.array.core.asarray(arr)\n",
    "\n",
    "    # Validate and normalize axis.\n",
    "    arr.shape[axis]\n",
    "    axis = len(arr.shape[:axis])\n",
    "\n",
    "    # Rechunk so that analyze is applied over the full axis.\n",
    "    arr = arr.rechunk(arr.chunks[:axis] + (arr.shape[axis:axis + 1], ) +\n",
    "                      arr.chunks[axis + 1:])\n",
    "\n",
    "    # Test out some data with the function.\n",
    "    test_data = np.ones(args[0].shape[1], dtype=arr.dtype)\n",
    "    test_result = np.array(func1d(test_data, *args, **kwargs))\n",
    "\n",
    "    # Map analyze over the data to get the result\n",
    "    # Adds other axes as needed.\n",
    "    result = arr.map_blocks(\n",
    "        _apply_along_axis,\n",
    "        name=dask.utils.funcname(func1d) + '-along-axis',\n",
    "        dtype=test_result.dtype,\n",
    "        chunks=(arr.chunks[:axis] + test_result.shape + arr.chunks[axis + 1:]),\n",
    "        drop_axis=axis,\n",
    "        new_axis=list(range(axis, axis + test_result.ndim, 1)),\n",
    "        func1d=func1d,\n",
    "        func1d_axis=axis,\n",
    "        func1d_args=args,\n",
    "        func1d_kwargs=kwargs,\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 42s, sys: 41.3 s, total: 11min 23s\n",
      "Wall time: 26min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ssh_rechunk = ssh.chunk(dask_array_rechunk(ssh))\n",
    "future = apply_along_axis(pytide.WaveTable.harmonic_analysis, 0, ssh_rechunk,\n",
    "                          *(f, vu))\n",
    "analysis = future.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182011"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(future.dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.92 s, sys: 2.9 s, total: 9.82 s\n",
      "Wall time: 9.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "amp_da=xr.DataArray(np.absolute(analysis),dims={'freq','y','x'},name='tidal-amp')\n",
    "amp_da.to_netcdf(path='/work/ALT/odatis/eNATL60/outputs/pytide/eNATL60-BLBT02_tidal_amp_y2009m07-m09.nc',engine='scipy')\n",
    "\n",
    "phase_da=xr.DataArray(np.angle(analysis),dims={'freq','y','x'},name='tidal-phase')\n",
    "phase_da.to_netcdf(path='/work/ALT/odatis/eNATL60/outputs/pytide/eNATL60-BLBT02_tidal_phase_y2009m07-m09.nc',engine='scipy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nt=len(time)\n",
    "nwaves, ni, nj = analysis.shape\n",
    "for k in np.arange(len(time)):\n",
    "    ttide = wt.tide_from_mapping(\n",
    "    time[k].astype('datetime64[s]').astype('float64'),\n",
    "    analysis.reshape(nwaves, ni*nj)).reshape(ni, nj)\n",
    "    ttide_da=xr.DataArray(ttide,dims={'y','x'},name='tide')\n",
    "    ttide_da.to_netcdf(path='/work/ALT/odatis/eNATL60/outputs/pytide/eNATL60-BLBT02_tide_k'+str(k)+'.nc',engine='scipy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytide",
   "language": "python",
   "name": "pytide"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
